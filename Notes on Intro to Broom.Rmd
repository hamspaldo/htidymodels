---
title: "Checking out Broom Vignette"
author: "Hamish Spalding"
date: "31/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("pacman")
pacman::p_load(tidyverse, AzureStor,glue, vroom)
key ="BNOTTHEREALKEYFORSTORAGE==" # set this and use with glue below. get from: Y:\Data\TfNSW\1_data\blob\train-occupancy-feb-2017.txt
key="Xpve==" #DELETED
```


## Intro to Broom

Following Intro to Broom at this link:
<https://cran.r-project.org/web/packages/broom/vignettes/broom.html>

**broom** package description is that "broom Converts Statistical Nalysis Objects into TIdy Tibbles."  

**broom use**s mpg data, so maybe try follow example with wome some different data. Iris is too common.   

```{r}
#Get some data from here - requires registration
#https://opendata.transport.nsw.gov.au/dataset/train-occupancy-nov-2016-feb-2017

```


#Get some data from here - requires registration  

https://opendata.transport.nsw.gov.au/dataset/train-occupancy-nov-2016-feb-2017

-blob storage (need to paste correct key - see if can make data public - require TfNSW permission)
-upload .csv to R and then spark

```{r}
pacman::p_load(SparkR,sparklyr)

SparkR::sparkR.session() # https://docs.databricks.com/spark/latest/sparkr/rstudio.html#get-started-with-rstudio-server-open-source
library(sparklyr)
sc <- spark_connect(method = "databricks")
src_tbls(sc)

##Access the blob storage
bl_endp_key <- storage_endpoint("https://trainsriskdata.blob.core.windows.net/trains-ae-riskdiv-opaltrainoccupancy-test-csv", key={key})
list_storage_containers(bl_endp_key) # $`trains-ae-riskdiv-opaltrainoccupancy-test-csv`

##Download files from lob storage

library(AzureStor)
#bl_endp_key <- storage_endpoint("https://mystorage.blob.core.windows.net", key="access_key")
bl_endp_key <- AzureStor::storage_endpoint("https://trainsriskdata.blob.core.windows.net", key={key})
AzureStor::list_storage_containers(bl_endp_key)
# gives the containers:
# $inputdata
# $shem24092019
# $`trains-ae-riskdiv-opaltrainoccupancy-test-csv`

cont <- storage_container(bl_endp_key, "shem24092019")
storage_download(cont, "/safety.csv", "~/htidymodels/safety.csv")


# try readr
safety <-readr::read_csv("~/htidymodels/safety.csv", col_types = cols(    #imports .csv to R dataframe
  .default = col_character(),
  `Incident Source System` = col_double(),
  `Organisational Unit ID` = col_double(),
  `Track Type` = col_double(),
  Kilometrage = col_double(),
  `Classification - Primary OC-G1 Code` = col_double(),
  `Classification - Notifiable Occurrence Category` = col_double(),
  `Near Miss` = col_character(), # are all NA so set to char()
  HPI = col_character(), # are all NA so set to char()
  `Review Responsible Person` = col_logical(),
  `Review Person Name` = col_logical(),
  `RCS - Internal Responsible Party Org. ID` = col_double(),
  `Network Maint. - Rail Infrastructure Maintainer` = col_double(),
  `Network Maint. - Primary Rail Infrastructure Manager` = col_double()
))

# try vrooom

safetyvroom <-vroom::vroom("~/htidymodels/safety.csv", col_types = cols(    #imports .csv to R dataframe
  .default = col_character(),
  `Incident Source System` = col_double(),
  `Organisational Unit ID` = col_double(),
  `Track Type` = col_double(),
  Kilometrage = col_double(),
  `Classification - Primary OC-G1 Code` = col_double(),
  `Classification - Notifiable Occurrence Category` = col_double(),
  `Near Miss` = col_character(), # are all NA so set to char()
  HPI = col_character(), # are all NA so set to char()
  `Review Responsible Person` = col_logical(),
  `Review Person Name` = col_logical(),
  `RCS - Internal Responsible Party Org. ID` = col_double(),
  `Network Maint. - Rail Infrastructure Maintainer` = col_double(),
  `Network Maint. - Primary Rail Infrastructure Manager` = col_double()
))


# try data.table
p_load("data.table")
safetyvfread <-data.table::fread("~/htidymodels/safety.csv")


sparklyr::sdf_copy_to(sc,safety) # copies R object into spark.
src_tbls(sc) # shows safety table is now on the spark cluster

cont2 <- storage_container(bl_endp_key, "trains-ae-riskdiv-opaltrainoccupancy-test-csv")
storage_download(cont2, "/train-occupancy-feb-2017.csv", "~/htidymodels/trains-ae-riskdiv-opaltrainoccupancy-test.csv")

opaltrainoccupancy <- readr::read_csv("~/htidymodels/trains-ae-riskdiv-opaltrainoccupancy-test.csv", col_types = cols( 
  day = col_double(),
  Actual.Stop.Station = col_character(),
  Actual.Station.Arrv.Time = col_datetime(format = ""),
  Actual.Station.Dprt.Time = col_datetime(format = ""),
  Segment.Direction = col_character(),
  Trip.Name = col_character(),
  Service.Line = col_character(),
  Orig..Station = col_character(),
  Dest..Station = col_character(),
  Leading.Set.Type = col_character(),
  Node.Seq.Order = col_double(),
  Actual.Station.Dprt.Time.Band = col_character(),
  `Occupancy Status` = col_character(),
  `Occupancy Range` = col_character()
)) #imports .csv to R dataframe

sparklyr::sdf_copy_to(sc,opaltrainoccupancy) # copies R object into spark. Alternative: sparklyr::copy_to(sc, opaltrainoccupancy, overwrite = TRUE)

#p_load(arrow)
#arrow::install_arrow() # to install arrow runtime libraries
#arrow::write_feather(safety, "~/htidymodels/safety.feather") # doesn't work as need C++ libraries..https://arrow.apache.org/install/
#safetyarrow <- arrow::write_feather("~/htidymodels/safety.feather")
#safetyarrow <- arrow::read_feather("~/htidymodels/safety.feather")

src_tbls(sc)


#####################################################


```


# rbanchmarks: readr::read_csv() vs vroom::vroom() vs data.table::fread()

```{r}

library(pacman)
p_load(rbenchmark)
# try readr

benchmark("readr" = {
        
      safety <-readr::read_csv("~/htidymodels/safety.csv", col_types = cols(    #imports .csv to R dataframe
        .default = col_character(),
        `Incident Source System` = col_double(),
        `Organisational Unit ID` = col_double(),
        `Track Type` = col_double(),
        Kilometrage = col_double(),
        `Classification - Primary OC-G1 Code` = col_double(),
        `Classification - Notifiable Occurrence Category` = col_double(),
        `Near Miss` = col_character(), # are all NA so set to char()
        HPI = col_character(), # are all NA so set to char()
        `Review Responsible Person` = col_logical(),
        `Review Person Name` = col_logical(),
        `RCS - Internal Responsible Party Org. ID` = col_double(),
        `Network Maint. - Rail Infrastructure Maintainer` = col_double(),
        `Network Maint. - Primary Rail Infrastructure Manager` = col_double()
      ))
                  },
      "vroom" = {
      # try vrooom
      #install.packages("vroom")
      library(vroom)
      safetyvroom <-vroom::vroom("~/htidymodels/safety.csv", col_types = cols(    #imports .csv to R dataframe
        .default = col_character(),
        `Incident Source System` = col_double(),
        `Organisational Unit ID` = col_double(),
        `Track Type` = col_double(),
        Kilometrage = col_double(),
        `Classification - Primary OC-G1 Code` = col_double(),
        `Classification - Notifiable Occurrence Category` = col_double(),
        `Near Miss` = col_character(), # are all NA so set to char()
        HPI = col_character(), # are all NA so set to char()
        `Review Responsible Person` = col_logical(),
        `Review Person Name` = col_logical(),
        `RCS - Internal Responsible Party Org. ID` = col_double(),
        `Network Maint. - Rail Infrastructure Maintainer` = col_double(),
        `Network Maint. - Primary Rail Infrastructure Manager` = col_double()
      ))
      },
      "fread" ={
        safetyvfread <-data.table::fread("~/htidymodels/safety.csv") # https://rdatatable.gitlab.io/data.table/index.html
        # https://h2oai.github.io/db-benchmark/
      }
)

```



```{r}
opaltrainoccupancy %>% tally()

origin <- unique(opaltrainoccupancy$Orig..Station)
origin.df <- base::as.data.frame(origin) %>%
  mutate(station=as.character(origin)) %>%
  select(station)

dest <- unique(opaltrainoccupancy$Dest..Station)
dest.df <- base::as.data.frame(dest) %>%
  mutate(station=as.character(dest)) %>%
  select(station)

bind_rows(origin.df,dest.df) %>%
  unique()


```

# what data to model

Will try to merge **safety** incident data on to **Opal** data on station
- when combining source and destination on opal data there are 78 unique stations 
- here we see that merging the two data sets in R is too large.

```{r}
names(opaltrainoccupancy)
glimpse(opaltrainoccupancy)
unique(opaltrainoccupancy$day) # 28 days of data
opaltrainoccupancy$Actual.Stop.Station
opaltrainoccupancy$`Occupancy Range`
opaltrainoccupancy$`Occupancy Status`
opaltrainoccupancy$Actual.Stop.Station

safetysnake <-janitor::clean_names(safety,case = c("snake")) %>%
  dplyr::rename(orig_station = location_name)

#glimpse(safetysnake)
#unique(safetysnake$location_name)

library(pacman)
p_load(janitor)
opal<-janitor::clean_names(opaltrainoccupancy,case = c("snake"))
names(opal)
glimpse(opal)
ggplot2::ggplot(data = opaltrainoccupancy) + aes(x=day, y=actual_stop_station)

ggplot(data = opaltrainoccupancy, aes(x = actual_station_arrv_time, y = continuousvar))+
  geom_line(color = "#00AFBB", size = 2)

```

work out which stations have the highest occupancy and how this relates to the count of safety incidents

merge safetysnake and opal by station
```{r}

opal %>% dplyr::left_join(safetysnake, by= "orig_station") # Error: cannot allocate vector of size 5.8 Gb

```

so Error: cannot allocate vector of size 5.8 Gb means that we have to use spark cluster..

- both opaltrainoccupancy and safety are on the cluster..

try using dplyr on cluster

```{r}
print(paste0("N Cores in use: ", sc$config$sparklyr.connect.cores.local))
```

```{r}
src_tbls(sc)
# create a reference to the spark tables in cache using the dplyr::tbl() function
opaltrainoccupancy <- dplyr::tbl(sc,"opaltrainoccupancy")
safety <- dplyr::tbl(sc,"safety")

glimpse(opaltrainoccupancy)
# of interest
#opaltrainoccupancy$Actual.Stop.Station
#opaltrainoccupancy$`Occupancy Range`
#opaltrainoccupancy$`Occupancy Status`
#opaltrainoccupancy$Actual.Stop.Station

print(paste0("N Cores in use: ", sc$config$sparklyr.cores.local))

opaltrainoccupancytest <- sparklyr::spark_apply(opaltrainoccupancy,
                                      function(df) janitor::clean_names(df,case = c("snake"))
                                      )

opaltrainoccupancy%>%
  sparklyr::spark_apply(function(e) head(e, 1))



safetysnake <-janitor::clean_names(safety,case = c("snake")) %>%
  dplyr::rename(orig_station = location_name)

opal<-janitor::clean_names(opaltrainoccupancy,case = c("snake"))

#glimpse(safetysnake)
#unique(safetysnake$location_name)



spark_apply(
  iris_tbl,
  function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e))
)

spark_apply(
  safety,
  function(e) janitor::clean_names(opaltrainoccupancy,case = c("snake"))
)

```


try using sparklyr on cluster

```{r}
src_tbls(sc)

## alternative using sparklyr:: check?
opaltrainoccupancy <- sparklyr::tbl_cache(sc,"opaltrainoccupancy")
```




## get some BOM data

```{r}
if (!require("remotes")) {
  install.packages("remotes", repos = "http://cran.rstudio.com/")
  library("remotes")
}

install_github("ropensci/bomrang", build_vignettes = TRUE)

library(bomrang)
```


##

Notes to follow up on
https://spark.rstudio.com/
https://www.r-bloggers.com/using-spark-from-r-for-performance-with-arbitrary-code-part-1-spark-sql-translation-custom-functions-and-arrow/
