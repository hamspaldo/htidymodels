---
title: "Checking out Broom Vignette"
author: "Hamish Spalding"
date: "31/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("pacman")
pacman::p_load(tidyverse, AzureStor,glue)
key ="BNOTTHEREALKEYFORSTORAGE==" # set this and use with glue below. get from: Y:\Data\TfNSW\1_data\blob\train-occupancy-feb-2017.txt
key="Xpvem==" #DELETED
```


## Intro to Broom

Following Intro to Broom at this link:
<https://cran.r-project.org/web/packages/broom/vignettes/broom.html>

**broom** package description is that "broom Converts Statistical Nalysis Objects into TIdy Tibbles."  

**broom use**s mpg data, so maybe try follow example with wome some different data. Iris is too common.   

```{r}
#Get some data from here - requires registration
#https://opendata.transport.nsw.gov.au/dataset/train-occupancy-nov-2016-feb-2017

```


#Get some data from here - requires registration  

https://opendata.transport.nsw.gov.au/dataset/train-occupancy-nov-2016-feb-2017

-blob storage (need to paste correct key - see if can make data public - require TfNSW permission)
-upload .csv to R and then spark

```{r}
pacman::p_load(SparkR,sparklyr)

SparkR::sparkR.session() # https://docs.databricks.com/spark/latest/sparkr/rstudio.html#get-started-with-rstudio-server-open-source
library(sparklyr)
sc <- spark_connect(method = "databricks")
src_tbls(sc)

##Access the blob storage
bl_endp_key <- storage_endpoint("https://trainsriskdata.blob.core.windows.net/trains-ae-riskdiv-opaltrainoccupancy-test-csv", key={key})
list_storage_containers(bl_endp_key) # $`trains-ae-riskdiv-opaltrainoccupancy-test-csv`

##Download files from lob storage

library(AzureStor)
#bl_endp_key <- storage_endpoint("https://mystorage.blob.core.windows.net", key="access_key")
bl_endp_key <- AzureStor::storage_endpoint("https://trainsriskdata.blob.core.windows.net", key={key})
AzureStor::list_storage_containers(bl_endp_key)
# gives the containers:
# $inputdata
# $shem24092019
# $`trains-ae-riskdiv-opaltrainoccupancy-test-csv`

cont <- storage_container(bl_endp_key, "shem24092019")
storage_download(cont, "/safety.csv", "~/htidymodels/safety.csv")

safety <-readr::read_csv("~/htidymodels/safety.csv", col_types = cols(    #imports .csv to R dataframe
  .default = col_character(),
  `Incident Source System` = col_double(),
  `Organisational Unit ID` = col_double(),
  `Track Type` = col_double(),
  Kilometrage = col_double(),
  `Classification - Primary OC-G1 Code` = col_double(),
  `Classification - Notifiable Occurrence Category` = col_double(),
  `Near Miss` = col_character(), # are all NA so set to char()
  HPI = col_character(), # are all NA so set to char()
  `Review Responsible Person` = col_logical(),
  `Review Person Name` = col_logical(),
  `RCS - Internal Responsible Party Org. ID` = col_double(),
  `Network Maint. - Rail Infrastructure Maintainer` = col_double(),
  `Network Maint. - Primary Rail Infrastructure Manager` = col_double()
))

sparklyr::sdf_copy_to(sc,safety) # copies R object into spark.

cont2 <- storage_container(bl_endp_key, "trains-ae-riskdiv-opaltrainoccupancy-test-csv")
storage_download(cont2, "/train-occupancy-feb-2017.csv", "~/htidymodels/trains-ae-riskdiv-opaltrainoccupancy-test.csv")

opaltrainoccupancy <- readr::read_csv("~/htidymodels/trains-ae-riskdiv-opaltrainoccupancy-test.csv", col_types = cols( 
  day = col_double(),
  Actual.Stop.Station = col_character(),
  Actual.Station.Arrv.Time = col_datetime(format = ""),
  Actual.Station.Dprt.Time = col_datetime(format = ""),
  Segment.Direction = col_character(),
  Trip.Name = col_character(),
  Service.Line = col_character(),
  Orig..Station = col_character(),
  Dest..Station = col_character(),
  Leading.Set.Type = col_character(),
  Node.Seq.Order = col_double(),
  Actual.Station.Dprt.Time.Band = col_character(),
  `Occupancy Status` = col_character(),
  `Occupancy Range` = col_character()
)) #imports .csv to R dataframe

sparklyr::sdf_copy_to(sc,opaltrainoccupancy) # copies R object into spark. Alternative: sparklyr::copy_to(sc, opaltrainoccupancy, overwrite = TRUE)



src_tbls(sc)


#####################################################


```

```{r}
opaltrainoccupancy %>% tally()

origin <- unique(opaltrainoccupancy$Orig..Station)
origin.df <- base::as.data.frame(origin) %>%
  mutate(station=as.character(origin)) %>%
  select(station)

dest <- unique(opaltrainoccupancy$Dest..Station)
dest.df <- base::as.data.frame(dest) %>%
  mutate(station=as.character(dest)) %>%
  select(station)

bind_rows(origin.df,dest.df) %>%
  unique()


```

## get some BOM data

```{r}
if (!require("remotes")) {
  install.packages("remotes", repos = "http://cran.rstudio.com/")
  library("remotes")
}

install_github("ropensci/bomrang", build_vignettes = TRUE)

library(bomrang)
```


##

Notes to follow up on
https://spark.rstudio.com/
https://www.r-bloggers.com/using-spark-from-r-for-performance-with-arbitrary-code-part-1-spark-sql-translation-custom-functions-and-arrow/
